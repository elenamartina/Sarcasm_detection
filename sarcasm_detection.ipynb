{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sarcasm Detection in News Headlines:\n",
    "## A Binary Classification Problem\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Statement of the Problem\n",
    "\n",
    "Sarcasm Detection is a challenging problem in the field of sentiment analysis, in part due to the lack intonation and facial expression. Sarcastic sentences can be used in a diverse range of topics, and they can take disparate grammatical structures. Also, in order to detect sarcasm, usually one has to have prior knowledge about the subject (which might not always be available): many sentences are not sarcastic by themselves, but they become in a particular context.   \n",
    "\n",
    "The aim of this project is to build a Deep Learning Neural Network model capable of detecting sarcasm in the Headings provided in the **\"News Headlines Dataset For Sarcasm Detection\" Kaggle** <a href=\"https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection\"> dataset. </a>\n",
    "\n",
    "\n",
    "## 1.2 About the Dataset\n",
    "\n",
    "\"Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision, but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\" <a href=\"https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home\"> [Kaggle, Dataset Overview] </a>  \n",
    "\n",
    "This dataset, as it was created by collecting both sarcastic and non-sarcastic news headlines (respectively from TheOnion, which aims at producing sarcastic versions of current events, and from HuffPost), has many advantages over other datasets. The news headlines are indeed self-contained, written in a formal manner and without spelling mistakes, which increases the chance of finding pre-trained embeddings.\n",
    "\n",
    "\n",
    "### Content\n",
    "\n",
    "Each record consists of three attributes:\n",
    "\n",
    "- `is_sarcastic`: 1 if the record is sarcastic otherwise 0\n",
    "- `headline:` the headline of the news article\n",
    "- `article_link:` link to the original news article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods\n",
    "---\n",
    "\n",
    "# 2.1 Data Preprocessing\n",
    "\n",
    "## 2.1.1 Reading the Data\n",
    "\n",
    "The data is stored in a .json file; the following code stores the file content in the list \"data\". All the Headlines are then separately stored in the \"headlines\" list, which will represent our training and test data (of course after some data preprocessing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline print example:  j.k. rowling wishes snape happy birthday in the most magical way\n"
     ]
    }
   ],
   "source": [
    "def parseJson(fname):\n",
    "    for line in open(fname, 'r'):\n",
    "        yield eval(line)\n",
    "\n",
    "        \n",
    "data = list(parseJson('./Sarcasm_Headlines_Dataset.json'))\n",
    "data_size = len(data) # 26709\n",
    "\n",
    "headlines = []\n",
    "\n",
    "# store all the Headlines in the Headlines list\n",
    "for i in range(data_size):\n",
    "    headlines.append(data[i].get('headline'))\n",
    "\n",
    "\n",
    "print('Headline print example: ', headlines[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Text Preprocessing\n",
    "\n",
    "### Fitting the Tokenizer on the documents\n",
    "- Keras provides utility classes in order to preprocess text.  \n",
    "- Here the Tokenizer class allow to preprocess a text corpus, in this case by turning each headline into a sequence of integers (indeces corresponding to a specific word).  \n",
    "- The argument num_word=10000 means the tokenizer will keep only the 10'000 most frequent occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing import text\n",
    "\n",
    "\n",
    "# create an instance of the Keras Tokenizer class\n",
    "t = text.Tokenizer(num_words=10000)\n",
    "\n",
    "# fit the tokenizer on the data\n",
    "t.fit_on_texts(headlines)\n",
    "\n",
    "#print(t.word_index) # prints a dictionary of words and their corresponding index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset in Train and Test Data\n",
    "\n",
    "- Assigning and fitting the data respectively to the Training and Test Data.\n",
    "- The training sizes I experimented with were 50%, 75% and 85% of the total data.  \n",
    "\n",
    "\n",
    "- The variables train_data and test_data are lists of headlines;\n",
    "- The variables train_labels and test_labels are lists of 0s and 1s, where 0 stands for non-sarcastic and 1 for sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size = int(data_size*0.5) # 13354\n",
    "#train_size = int(data_size*0.75) # 20031\n",
    "train_size = int(data_size*0.85) # 22702\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for x in t.texts_to_sequences_generator(headlines[:train_size]):\n",
    "    train_data.append(x)\n",
    "\n",
    "t.fit_on_sequences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "for x in t.texts_to_sequences_generator(headlines[train_size:]):\n",
    "    test_data.append(x)\n",
    "    \n",
    "t.fit_on_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_labels.append(data[i].get('is_sarcastic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "for i in range(train_size, data_size):\n",
    "    test_labels.append(data[i].get('is_sarcastic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because I discarded any words that are not in the most frequent 10'000, no word index will exceed 10'000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "mx = 0\n",
    "for sequence in train_data:\n",
    "    if len(sequence) is not 0:\n",
    "        if mx < max([max(sequence)]):\n",
    "            mx = max([max(sequence)])\n",
    "\n",
    "print(mx) # 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Vectorization\n",
    "\n",
    "- One-Hot encoding turns the lists into 2D tensors containing binary vectors of 0s and 1s.\n",
    "- Turning the lists of integers into `tensors` makes it possible to feed them into a neural network and use a `Dense` layer as the first layer, as it can handle floating-point vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros( (len(sequences), dimension) )\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Building the Model\n",
    "---\n",
    "\n",
    "Except for the number of units in the hidden layers, the models are built with the same architecture and features, as shown in the table below:\n",
    "\n",
    "| Parameter | Value |\n",
    "| --- | --- |\n",
    "| Hidden Layers | 2 |\n",
    "|Activation Function | `sigmoid`|\n",
    "|Loss Function | `binary_crossentropy`|\n",
    "|Optimizer | `rmsprop` |\n",
    "|Metrics | `acc` |\n",
    "\n",
    "\n",
    ">- The **intermediate layers** are `Dense` layers that use `relu` (rectified linear unit) as their activation function, which is meant to zero out negative values;\n",
    ">- The **final layer** is a single-unit layer with a `sigmoid` activation function, which will output a probability between 0 and 1;  \n",
    "\n",
    ">- The **loss function** used is the `binary_crossentropy`, which is usually the best choice when dealing with models that output probabilities;\n",
    ">- The **optimizer** used is the `rmsprop` (Root Mean Square Propagation).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "n_units = 16\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(n_units, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(n_units, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Evaluation Protocol\n",
    "---\n",
    "### Hold-out Validation Set\n",
    "The **validation set** will set apart 10'000 samples from the original training data, in order to monitor the accuracy of the model on new data during the training. It will be a good-enough Evaluation Protocol as we have plenty of data for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Training\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.1 Hyperparameter Tuning\n",
    "\n",
    "- I focused on changing the following parameters:\n",
    "    - The number of units of each layer;\n",
    "    - The learning rate;\n",
    "    - The batch size;\n",
    "    - The number of epochs, depending on the performance of each run.\n",
    "    \n",
    "\n",
    "| Parameter | Values | Final Model Value |\n",
    "| --- | --- | --- |\n",
    "|Units | {4, 16, 32, 256} | 16 |\n",
    "|Learning Rate | {0.003, 0.001, 0.0005, 0.00025} | 0.001 |\n",
    "|Batch Size | {16, 64, 512, 1024, 2048} | 1024 |\n",
    "|Epochs | 5, 10, 20, 40, 60 |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12702 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12702/12702 [==============================] - 9s 723us/step - loss: 0.6642 - acc: 0.6736 - val_loss: 0.6262 - val_acc: 0.7979\n",
      "Epoch 2/20\n",
      "12702/12702 [==============================] - 9s 745us/step - loss: 0.5812 - acc: 0.8555 - val_loss: 0.5633 - val_acc: 0.8270\n",
      "Epoch 3/20\n",
      "12702/12702 [==============================] - 8s 608us/step - loss: 0.5061 - acc: 0.8849 - val_loss: 0.5092 - val_acc: 0.8374\n",
      "Epoch 4/20\n",
      "12702/12702 [==============================] - 7s 588us/step - loss: 0.4391 - acc: 0.9008 - val_loss: 0.4624 - val_acc: 0.8423\n",
      "Epoch 5/20\n",
      "12702/12702 [==============================] - 7s 584us/step - loss: 0.3800 - acc: 0.9107 - val_loss: 0.4244 - val_acc: 0.8447\n",
      "Epoch 6/20\n",
      "12702/12702 [==============================] - 7s 582us/step - loss: 0.3296 - acc: 0.9199 - val_loss: 0.3957 - val_acc: 0.8460\n",
      "Epoch 7/20\n",
      "12702/12702 [==============================] - 8s 593us/step - loss: 0.2876 - acc: 0.9262 - val_loss: 0.3750 - val_acc: 0.8464\n",
      "Epoch 8/20\n",
      "12702/12702 [==============================] - 7s 579us/step - loss: 0.2528 - acc: 0.9335 - val_loss: 0.3617 - val_acc: 0.8473\n",
      "Epoch 9/20\n",
      "12702/12702 [==============================] - 7s 577us/step - loss: 0.2240 - acc: 0.9387 - val_loss: 0.3550 - val_acc: 0.8468\n",
      "Epoch 10/20\n",
      "12702/12702 [==============================] - 7s 580us/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3507 - val_acc: 0.8451\n",
      "Epoch 11/20\n",
      "12702/12702 [==============================] - 8s 604us/step - loss: 0.1787 - acc: 0.9490 - val_loss: 0.3516 - val_acc: 0.8447\n",
      "Epoch 12/20\n",
      "12702/12702 [==============================] - 8s 656us/step - loss: 0.1604 - acc: 0.9547 - val_loss: 0.3590 - val_acc: 0.8441\n",
      "Epoch 13/20\n",
      "12702/12702 [==============================] - 8s 622us/step - loss: 0.1447 - acc: 0.9587 - val_loss: 0.3634 - val_acc: 0.8428\n",
      "Epoch 14/20\n",
      "12702/12702 [==============================] - 8s 632us/step - loss: 0.1307 - acc: 0.9627 - val_loss: 0.3729 - val_acc: 0.8417\n",
      "Epoch 15/20\n",
      "12702/12702 [==============================] - 8s 629us/step - loss: 0.1181 - acc: 0.9665 - val_loss: 0.3848 - val_acc: 0.8402\n",
      "Epoch 16/20\n",
      "12702/12702 [==============================] - 8s 617us/step - loss: 0.1069 - acc: 0.9706 - val_loss: 0.4002 - val_acc: 0.8401\n",
      "Epoch 17/20\n",
      "12702/12702 [==============================] - 7s 565us/step - loss: 0.0968 - acc: 0.9743 - val_loss: 0.4138 - val_acc: 0.8387\n",
      "Epoch 18/20\n",
      "12702/12702 [==============================] - 9s 699us/step - loss: 0.0873 - acc: 0.9766 - val_loss: 0.4321 - val_acc: 0.8375\n",
      "Epoch 19/20\n",
      "12702/12702 [==============================] - 7s 586us/step - loss: 0.0788 - acc: 0.9791 - val_loss: 0.4539 - val_acc: 0.8345\n",
      "Epoch 20/20\n",
      "12702/12702 [==============================] - 8s 624us/step - loss: 0.0709 - acc: 0.9806 - val_loss: 0.4732 - val_acc: 0.8336\n",
      "4007/4007 [==============================] - 2s 424us/step\n",
      "Results:  [0.4733459255642983, 0.8300474170350898]\n",
      "Accuracy: 83.00%\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = n_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"Results: \", results)\n",
    "print(\"Accuracy: %.2f%%\" % (results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Reducing Overfitting\n",
    "\n",
    "\n",
    "I experimented with many methods to reduce overfitting:\n",
    "\n",
    "- `Reducing network size` worked in terms of reducing overfitting, altough it caused the accuracy to dramatically drop;\n",
    "- `L1 Weight Regularization` performed poorly;\n",
    "- `L2 Weight Regularization` had a reasonable performance and reduced overfitting, although the best validation it achieved was just ~45%.\n",
    "- Introducing `Dropout` appeared to be the best solution to reduce overfitting while maintaining a good validation loss in the model.\n",
    "- The Dropout rates tested were 0.1, 0.3 and 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1 L2 Weight Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12702 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12702/12702 [==============================] - 8s 610us/step - loss: 0.7013 - acc: 0.6447 - val_loss: 0.6653 - val_acc: 0.7106\n",
      "Epoch 2/20\n",
      "12702/12702 [==============================] - 8s 598us/step - loss: 0.6327 - acc: 0.7818 - val_loss: 0.6174 - val_acc: 0.7890\n",
      "Epoch 3/20\n",
      "12702/12702 [==============================] - 7s 580us/step - loss: 0.5787 - acc: 0.8409 - val_loss: 0.5779 - val_acc: 0.8126\n",
      "Epoch 4/20\n",
      "12702/12702 [==============================] - 9s 732us/step - loss: 0.5311 - acc: 0.8677 - val_loss: 0.5429 - val_acc: 0.8258\n",
      "Epoch 5/20\n",
      "12702/12702 [==============================] - 7s 574us/step - loss: 0.4879 - acc: 0.8844 - val_loss: 0.5126 - val_acc: 0.8377\n",
      "Epoch 6/20\n",
      "12702/12702 [==============================] - 7s 570us/step - loss: 0.4501 - acc: 0.8955 - val_loss: 0.4878 - val_acc: 0.8390\n",
      "Epoch 7/20\n",
      "12702/12702 [==============================] - 8s 610us/step - loss: 0.4180 - acc: 0.8999 - val_loss: 0.4686 - val_acc: 0.8402\n",
      "Epoch 8/20\n",
      "12702/12702 [==============================] - 7s 554us/step - loss: 0.3913 - acc: 0.9084 - val_loss: 0.4550 - val_acc: 0.8426\n",
      "Epoch 9/20\n",
      "12702/12702 [==============================] - 7s 552us/step - loss: 0.3693 - acc: 0.9129 - val_loss: 0.4456 - val_acc: 0.8422\n",
      "Epoch 10/20\n",
      "12702/12702 [==============================] - 7s 564us/step - loss: 0.3509 - acc: 0.9180 - val_loss: 0.4395 - val_acc: 0.8416\n",
      "Epoch 11/20\n",
      "12702/12702 [==============================] - 9s 705us/step - loss: 0.3349 - acc: 0.9227 - val_loss: 0.4361 - val_acc: 0.8428\n",
      "Epoch 12/20\n",
      "12702/12702 [==============================] - 7s 555us/step - loss: 0.3213 - acc: 0.9253 - val_loss: 0.4338 - val_acc: 0.8425\n",
      "Epoch 13/20\n",
      "12702/12702 [==============================] - 7s 556us/step - loss: 0.3091 - acc: 0.9313 - val_loss: 0.4333 - val_acc: 0.8432\n",
      "Epoch 14/20\n",
      "12702/12702 [==============================] - 7s 553us/step - loss: 0.2984 - acc: 0.9336 - val_loss: 0.4346 - val_acc: 0.8431\n",
      "Epoch 15/20\n",
      "12702/12702 [==============================] - 7s 552us/step - loss: 0.2881 - acc: 0.9386 - val_loss: 0.4360 - val_acc: 0.8420\n",
      "Epoch 16/20\n",
      "12702/12702 [==============================] - 7s 555us/step - loss: 0.2789 - acc: 0.9405 - val_loss: 0.4400 - val_acc: 0.8395\n",
      "Epoch 17/20\n",
      "12702/12702 [==============================] - 7s 553us/step - loss: 0.2702 - acc: 0.9458 - val_loss: 0.4405 - val_acc: 0.8399\n",
      "Epoch 18/20\n",
      "12702/12702 [==============================] - 7s 554us/step - loss: 0.2621 - acc: 0.9476 - val_loss: 0.4447 - val_acc: 0.8392\n",
      "Epoch 19/20\n",
      "12702/12702 [==============================] - 7s 557us/step - loss: 0.2546 - acc: 0.9515 - val_loss: 0.4471 - val_acc: 0.8388\n",
      "Epoch 20/20\n",
      "12702/12702 [==============================] - 7s 556us/step - loss: 0.2475 - acc: 0.9542 - val_loss: 0.4510 - val_acc: 0.8406\n",
      "4007/4007 [==============================] - 2s 422us/step\n",
      "Results:  [0.4483306345601673, 0.838283004741702]\n",
      "Accuracy: 83.83%\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "l2 = regularizers.l2(0.001)\n",
    "\n",
    "reg_l2_model = models.Sequential()\n",
    "reg_l2_model.add(layers.Dense(n_units, kernel_regularizer = l2, activation = 'relu', input_shape = (10000,)))\n",
    "reg_l2_model.add(layers.Dense(n_units, kernel_regularizer = l2, activation = 'relu'))\n",
    "reg_l2_model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "reg_l2_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reg_l2_history = reg_l2_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs = n_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "\n",
    "results = reg_l2_model.evaluate(x_test, y_test)\n",
    "print(\"Results: \", results)\n",
    "print(\"Accuracy: %.2f%%\" % (results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.2 Adding Dropout\n",
    "\n",
    "- This will be the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12702 samples, validate on 10000 samples\n",
      "Epoch 1/18\n",
      "12702/12702 [==============================] - 9s 731us/step - loss: 0.6828 - acc: 0.5780 - val_loss: 0.6665 - val_acc: 0.6336\n",
      "Epoch 2/18\n",
      "12702/12702 [==============================] - 8s 652us/step - loss: 0.6491 - acc: 0.6794 - val_loss: 0.6284 - val_acc: 0.7701\n",
      "Epoch 3/18\n",
      "12702/12702 [==============================] - 7s 552us/step - loss: 0.6071 - acc: 0.7405 - val_loss: 0.5862 - val_acc: 0.8085\n",
      "Epoch 4/18\n",
      "12702/12702 [==============================] - 7s 563us/step - loss: 0.5663 - acc: 0.7801 - val_loss: 0.5456 - val_acc: 0.8272\n",
      "Epoch 5/18\n",
      "12702/12702 [==============================] - 7s 554us/step - loss: 0.5261 - acc: 0.8050 - val_loss: 0.5067 - val_acc: 0.8366\n",
      "Epoch 6/18\n",
      "12702/12702 [==============================] - 8s 610us/step - loss: 0.4939 - acc: 0.8161 - val_loss: 0.4732 - val_acc: 0.8414\n",
      "Epoch 7/18\n",
      "12702/12702 [==============================] - 7s 557us/step - loss: 0.4551 - acc: 0.8359 - val_loss: 0.4432 - val_acc: 0.8408\n",
      "Epoch 8/18\n",
      "12702/12702 [==============================] - 7s 590us/step - loss: 0.4308 - acc: 0.8470 - val_loss: 0.4189 - val_acc: 0.8435\n",
      "Epoch 9/18\n",
      "12702/12702 [==============================] - 7s 564us/step - loss: 0.4023 - acc: 0.8570 - val_loss: 0.3999 - val_acc: 0.8439\n",
      "Epoch 10/18\n",
      "12702/12702 [==============================] - 8s 625us/step - loss: 0.3674 - acc: 0.8793 - val_loss: 0.3819 - val_acc: 0.8462\n",
      "Epoch 11/18\n",
      "12702/12702 [==============================] - 9s 724us/step - loss: 0.3491 - acc: 0.8836 - val_loss: 0.3701 - val_acc: 0.8472\n",
      "Epoch 12/18\n",
      "12702/12702 [==============================] - 9s 687us/step - loss: 0.3237 - acc: 0.8931 - val_loss: 0.3612 - val_acc: 0.8486\n",
      "Epoch 13/18\n",
      "12702/12702 [==============================] - 8s 641us/step - loss: 0.3049 - acc: 0.8999 - val_loss: 0.3549 - val_acc: 0.8483\n",
      "Epoch 14/18\n",
      "12702/12702 [==============================] - 7s 558us/step - loss: 0.2881 - acc: 0.9061 - val_loss: 0.3514 - val_acc: 0.8484\n",
      "Epoch 15/18\n",
      "12702/12702 [==============================] - 7s 561us/step - loss: 0.2760 - acc: 0.9120 - val_loss: 0.3494 - val_acc: 0.8494\n",
      "Epoch 16/18\n",
      "12702/12702 [==============================] - 7s 560us/step - loss: 0.2576 - acc: 0.9184 - val_loss: 0.3496 - val_acc: 0.8507\n",
      "Epoch 17/18\n",
      "12702/12702 [==============================] - 7s 558us/step - loss: 0.2449 - acc: 0.9227 - val_loss: 0.3502 - val_acc: 0.8490\n",
      "Epoch 18/18\n",
      "12702/12702 [==============================] - 7s 556us/step - loss: 0.2222 - acc: 0.9325 - val_loss: 0.3532 - val_acc: 0.8483\n",
      "4007/4007 [==============================] - 2s 404us/step\n",
      "Results:  [0.35560559869809194, 0.8445220863488895]\n",
      "Accuracy: 84.45%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 17\n",
    "dropout_rate = 0.5\n",
    "\n",
    "do_model = models.Sequential()\n",
    "do_model.add(layers.Dense(n_units, activation='relu', input_shape=(10000,)))\n",
    "do_model.add(layers.Dropout(dropout_rate))\n",
    "do_model.add(layers.Dense(n_units, activation='relu'))\n",
    "do_model.add(layers.Dropout(dropout_rate))\n",
    "do_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "do_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "do_history = do_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs = n_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "results = do_model.evaluate(x_test, y_test)\n",
    "print(\"Results: \", results)\n",
    "print(\"Accuracy: %.2f%%\" % (results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Results\n",
    "\n",
    "\n",
    "# 3.1 Hyperparameter tuning results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 Changing the Training Size\n",
    "\n",
    "\n",
    "The parameter I started testing was the amount of data to assign to the training data, a really essential parameter, as it represents the quantity of information the model will actually learn from.\n",
    "\n",
    "The training sizes I experimented with were 50%, 85%, and 75% of the total data.\n",
    "\n",
    "### The model trained on more data proved to naturally generalize better.\n",
    "\n",
    "| TRAINING SIZE | EPOCHS BEFORE OVERFITTING| BEST LOSS | BEST ACCURACY |\n",
    "| --- | --- | --- | --- |\n",
    "| 50% | 14 | ~45% | ~79% |\n",
    "| 75% | 11 | ~37% | ~83% |\n",
    "| **85%** | **7** | **~35%** | **~85%** |\n",
    "\n",
    ">As we can see from the observations in the table:\n",
    ">- Training the model with **50%** (the ratio used also in the IMDB movie review dataset) performed quite well in terms of Overfitting, although the values of the validations loss and validation accuracy werre poor;\n",
    ">- The **85%** ratio showed a 10% better validation loss and a 5% better validation accuracy compared to the same model with a 50% training size, even though it caused overfitting after just 7 epochs.  \n",
    ">\n",
    ">\n",
    ">- We can see from here that there is an inverse relationship between tranining size and overfitting, while the accuracy increments with the training size. This is proof that the model trained on more data proved to naturally generalize better.\n",
    ">- Achieving overfitting is a problem that can be resolved later with regularization; for now, we need to obtain a model with statistical power. \n",
    "\n",
    "#### The other parameters of this first naive model are shown in the table below.\n",
    "\n",
    "\n",
    "| PARAMETER | VALUE | PARAMETER | VALUE |\n",
    "| --------------- | --- | --------------- | --- |\n",
    "| Training Size   | 85% |Learning Rate   | 0.001 |\n",
    "| Units           | 16 | Layers          | 2 |\n",
    "| Batch Size      | 512 | N of Epochs     | 20 |\n",
    "| Dropout         | N/A | Weight Regularization | N/A |\n",
    "| Best Loss       | ~35% | Best Accuracy | ~85% |\n",
    "| . | ![title](images/loss_1.png) | . | ![title](images/acc_1.png) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 Tuning the Learning Rate η\n",
    "\n",
    "### The RMSProp default Lerning Rate (η = 0.001) resulted to be a valid choice.\n",
    "\n",
    "| LEARNING RATE | EPOCHS BEFORE OVERFITTING | BEST LOSS | BEST ACCURACY |\n",
    "| --- | --- | --- | --- |\n",
    "| 0.003 | 3 | ~37% | ~84% |\n",
    "| **0.001** | **7** | **~35%** | **~85%** |\n",
    "| 0.0005 | 16 | ~35% | ~85% |\n",
    "| 0.00025 | 32 | ~35% | ~85% |\n",
    "\n",
    ">As we can see from the observations in the table:\n",
    ">- **Low learning rates as η < 0.0005** converge smoothly and postpone overfitting, but they dramatically slow down the learning process and the running time needed, and therefore results to be not efficient.\n",
    ">- A **larger learning rate, such as η = 0.003,** speeds up the learning, but causes overfitting just after the first 3 epochs, result that we want to avoid.\n",
    ">- A valid choice turns out to be **η = 0.001**, as it doesn't differ from the other learning rates in terms of validation accuracy and loss, but it conpesate the overfitting and the learning speed.\n",
    "\n",
    "\n",
    "For now, we will therefore keep our first model displayed above in 3.1.1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3 Tuning the number of Hidden Layer Units\n",
    "\n",
    "\n",
    "### The model compensates performance and overfitting with 16 units:\n",
    "\n",
    "| N OF UNITS | EPOCHS BEFORE OVERFITTING | BEST LOSS | BEST ACCURACY |\n",
    "| --- | --- | --- | --- |\n",
    "| 4 | 24 | ~45% | ~80% |\n",
    "| **16** | **7** | **~35%** | **~85%** |\n",
    "| 32 | 6 | ~37% | ~84% |\n",
    "| 256 | 3 | ~36% | ~84% |\n",
    "\n",
    ">As we can see from the observations in the table:\n",
    ">- In a **lower capacity model**, such as the 4 units one, we can notice an example of `underfitting`: the model is not powerful enough and has not yet modeled all relevant patterns in the training data;\n",
    ">- Also in the similar problem of **classifying the IMDB movie review dataset**, a smaller network was modeled with 2 layers of 4 units each, in order to overcome overfitting. The accuracy of that model, though, never reached the same accuracy of the bigger 16-units model. On this dataset as well, we can see the same poor performance of the 4-units model, that underperforms the bigger model by a 10% more loss and a 5% less accuracy.  \n",
    ">\n",
    ">\n",
    ">- The model both performs better and delays overfitting with 16 hidden units: **a higher-dimensional representation space** allows the network to learn more complex representations, even though it makes the network more computationally expensive.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For now, we will therefore keep our first model displayed above in 3.1.1.\n",
    "\n",
    "\n",
    "#### In the graph below we can see an example of underfitting (before ~20/25 epochs) of a model with 4 units.\n",
    "\n",
    "\n",
    "|  .    | . |\n",
    "| --- | --- |\n",
    "| ![title](images/loss_2.png) | ![title](images/acc_2.png) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.4 Changing the Batch Size\n",
    "\n",
    "\n",
    "### Increasing the Batch Size improves the performance\n",
    "\n",
    "| BATCH SIZE | EPOCHS BEFORE OVERFITTING | BEST LOSS | BEST ACCURACY |\n",
    "| --- | --- | --- | --- |\n",
    "| 16 | 6 | ~40% | ~84% |\n",
    "| 64 | 5 | ~37% | ~83% |\n",
    "| 512 | 7 | ~35% | ~85% |\n",
    "| **1024** | **12** | **~34.5%** | **~85%** |\n",
    "| 2048 | 29 | ~39% | ~85% |\n",
    "\n",
    "\n",
    ">As we can see from the observations in the table:\n",
    ">- The model seems to respond nicely to the incrementation of the batch size: both its performance and it overfitting improves as the batch size increases.\n",
    ">- A Batch Size of 1024 seemed to be the best fit from the testing.\n",
    "\n",
    "\n",
    "#### We can see the graph of the improved model below.\n",
    "\n",
    "| PARAMETER | VALUE | PARAMETER | VALUE |\n",
    "| --------------- | --- | --------------- | --- |\n",
    "| Training Size   | 85% |Learning Rate   | 0.001 |\n",
    "| Units           | 16 | Layers          | 2 |\n",
    "| Batch Size      | 1024 | N of Epochs     | 12 |\n",
    "| Dropout         | N/A | Weight Regularization | N/A |\n",
    "| Best Loss       | ~34.5% | Best Accuracy | ~85% |\n",
    "| . | ![title](images/loss_3.png) | . | ![title](images/acc_3.png) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Reducing Overfitting: Tuning Dropout Rate\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2.1 Dropout vs L2 Regularization\n",
    "\n",
    "#### Dropout strictly outperformed L2 Regularization\n",
    "\n",
    "|  .    | . |\n",
    "| --- | --- |\n",
    "| ![title](images/reg.png) | . |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2.2  Tuning Dropout Rate\n",
    "\n",
    "#### Increasing Dropout maintains a good performance while largely delaying overfitting\n",
    "\n",
    "| DROPOUT RATE | EPOCHS BEFORE OVERFITTING | BEST LOSS | BEST ACCURACY |\n",
    "| --- | --- | --- | --- |\n",
    "| 0.1 | 11 | ~34.5% | ~85% |\n",
    "| 0.3 | 13 | ~35% | ~85% |\n",
    "| **0.5** | **17** | **~35%** | **~85%** |\n",
    "| . |![title](images/dropouts.png) | . |  . |\n",
    "\n",
    "\n",
    ">As we can see from the observations in the table:\n",
    ">- A **dropout rate of 0.1** is too small to actually see changes from the model without dropout;\n",
    ">- **dr = 0.5** is a good dropout rate choice: while maintaining the same performance of **dr = 0.3**, it doesn't start overfitting until 18 epochs.\n",
    "\n",
    "---\n",
    "#### The final model\n",
    "\n",
    "| PARAMETER | VALUE | PARAMETER | VALUE |\n",
    "| --------------- | --- | --------------- | --- |\n",
    "| Training Size   | 85% |Learning Rate   | 0.001 |\n",
    "| Units           | 16 | Layers          | 2 |\n",
    "| Batch Size      | 1024 | N of Epochs     | 17 |\n",
    "| Dropout         | 0.5 | Weight Regularization | N/A |\n",
    "| Best Loss       | ~35.5% | Best Accuracy | ~84.5% |\n",
    "| . | ![title](images/loss_4.png) | . | ![title](images/acc_4.png) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "---\n",
    "\n",
    "\n",
    "## 4.1 Conclusion\n",
    "\n",
    "\n",
    "- One of the parameters that showed greater changes in the performance of the model was the **training data size**. The difference between 50%, 75%, and 85% was remarkable, and showed how much better the model learns with more data, and therefore generalizes better.\n",
    "- Building a **higher-dimensional representation space** as well as increasing the **batch size** allowed the neural network to learn more complex representations and improved the performance on validation and delayed overfitting.  \n",
    "\n",
    "\n",
    "- The model achieved **without regularization** a `validation loss of ~34.5%` and a `validation accuracy of ~85%`, although it started overfitting after about 12 epochs.\n",
    "- After the **regularization**, we achieved almost the same performance, but removed overfitting for the first 18 epochs, which is overall a better performance.\n",
    "- The data seemed to perform quite well and to not be noisy; the **standard deviation** was always low, and the output never oscillated too much.\n",
    "\n",
    "\n",
    "> The final performance of this model resulted in a `validation loss of ~35%` and a `validation accuracy of ~85%`.\n",
    "> The model strictly outperforms the **baseline**, corresponding to a 50% accuracy.\n",
    "\n",
    "#### The final model is displayed in the table below.\n",
    "\n",
    "| PARAMETER | VALUE | PARAMETER | VALUE |\n",
    "| --------------- | --- | --------------- | --- |\n",
    "| Training Size   | 85% |Learning Rate   | 0.001 |\n",
    "| Units           | 16 | Layers          | 2 |\n",
    "| Batch Size      | 1024 | N of Epochs     | 17 |\n",
    "| Dropout         | 0.5 | Weight Regularization | N/A |\n",
    "| Best Loss       | ~35.5% | Best Accuracy | ~84.5% |\n",
    "| . | ![title](images/loss_4.png) | . | ![title](images/acc_4.png) |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 4.2 Improvements\n",
    "\n",
    "- One of the main improvements that can be done regards data pre-processing. For example, as punctuation might be important for sarcasm detection, encoding esclamation and question marks could be helpful, as well as removing stopwords that do not add much value to the model (f.e. this, me, there...)\n",
    "\n",
    "- Also, feeding more data to the model, as well as building a more complicated neural network could help reach a better performance: for example, a hybrid of LSTM and CNN achieved an error rate of only 0.10 on this same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
